{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sequential_functional_skel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##The Functional and Sequential Keras API##\n",
        "in this notebook we will explore some examples of the sequential api that we know, and the functional api which we will see for the first time today.  The sequential api is a useful tool, but it lacks some functionality that we may want to take advantage of when building more complicated networks."
      ],
      "metadata": {
        "id": "2IcraEbpZTuZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCVCgEJAZO_G"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Flatten, Conv2D, MaxPooling2D, Dense, Input, Concatenate\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import Image\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
        "\n",
        "x_train, x_test = np.expand_dims(x_train, axis=-1), np.expand_dims(x_test, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's take a look at the data (TODO)"
      ],
      "metadata": {
        "id": "_DoXw0u3bfsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = tf.image.resize(x_train[np.random.randint(0, len(x_train))], (256, 256))\n",
        "img = tf.keras.utils.array_to_img(img)\n",
        "img"
      ],
      "metadata": {
        "id": "dU-OECWLmqHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_validation_split(x_train, y_train, val_fraction=.2, shuffle=True):\n",
        "  # generate a random permutation of the indicies (shuffle the data)\n",
        "  # this ensures the distribution of examples is the same along the split\n",
        "  indices = np.random.permutation(x_train.shape[0])\n",
        "  # calculate the index where we wish to split\n",
        "  split_index = int(x_train.shape[0] * val_fraction)\n",
        "  # splite the permutation into sets of the desired size\n",
        "  train_indices, val_indices = indices[split_index:], indices[:split_index]\n",
        "  # return (x_train, y_train), (x_val, y_val)\n",
        "  return (x_train[train_indices], y_train[train_indices]), (x_train[val_indices], y_train[val_indices])"
      ],
      "metadata": {
        "id": "GsVSjKFEbBbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_val, y_val) = None # TODO\n",
        "# let's look at it again (TODO)\n"
      ],
      "metadata": {
        "id": "GXf2Y308cUiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_sequential_model(conv_filters,\n",
        "                           conv_size,\n",
        "                           dense_layers,\n",
        "                           image_size=(28, 28, 1),\n",
        "                           learning_rate=1e-3,\n",
        "                           n_classes=10,\n",
        "                           activation='selu'):\n",
        "  # create the model object\n",
        "  model = tf.keras.Sequential()\n",
        "  # add an input layer (this step is only needed for the summary)\n",
        "  model.add(Input(image_size))\n",
        "  # add the convolutional layers\n",
        "  for (filters, kernel) in zip(conv_filters, conv_size):\n",
        "    model.add(Conv2D(filters=filters, kernel_size=(kernel, kernel), activation=activation))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "  # flatten\n",
        "  model.add(Flatten())\n",
        "  # add dense layers\n",
        "  for neurons in dense_layers:\n",
        "    model.add(Dense(neurons, activation=activation))\n",
        "  # classification output\n",
        "  model.add(Dense(n_classes, activation=tf.keras.activations.softmax))\n",
        "  # optimizer\n",
        "  opt = tf.keras.optimizers.Nadam(learning_rate=learning_rate, \n",
        "                                  beta_1=0.9, beta_2=0.999,\n",
        "                                  epsilon=None, decay=0.99)\n",
        "  # compile the model\n",
        "  model.compile(loss='sparse_categorical_crossentropy',\n",
        "                optimizer=opt, \n",
        "                metrics=['categorical_accuracy'])\n",
        "  \n",
        "  # Generate an ASCII representation of the architecture\n",
        "  print(model.summary())\n",
        "  \n",
        "  return model\n",
        "  "
      ],
      "metadata": {
        "id": "T_eJY5Wee9V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_arguments = {  # TODO: define parameters\n",
        "    'conv_filters' : None,\n",
        "    'conv_size' : None,\n",
        "    'dense_layers' : None\n",
        "}\n",
        "\n",
        "seq_model = build_sequential_model(**model_arguments)\n",
        "# generate a graphical representation of the neural network\n",
        "plot_model(seq_model, show_shapes=True, show_layer_names=True, to_file='seq_model.png')\n",
        "# display the graphical representation\n",
        "Image('fun_model.png')"
      ],
      "metadata": {
        "id": "IJhJrlIjhzmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_functional_model(conv_filters,\n",
        "                           conv_size,\n",
        "                           dense_layers,\n",
        "                           image_size=(28, 28, 1),\n",
        "                           learning_rate=1e-3,\n",
        "                           n_classes=10,\n",
        "                           activation='selu'):\n",
        "  # define the input layer (required)\n",
        "  inputs = Input(image_size)\n",
        "  # set reference x separately to keep track of the input layer\n",
        "  x = inputs\n",
        "  # construct the convolutional part\n",
        "  for (filters, kernel) in zip(conv_filters, conv_size):\n",
        "    # each layer is a function of the previous layer, we can reuse reference x\n",
        "    x = Conv2D(filters=filters, kernel_size=(kernel, kernel), activation=activation)(x)\n",
        "    # pooling after a convolution (or two) is a standard simple technique\n",
        "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
        "  # flatten\n",
        "  x = Flatten()(x)\n",
        "  # construct the dense part\n",
        "  for neurons in dense_layers:\n",
        "   x = Dense(neurons, activation=activation)(x)\n",
        "  # classification output\n",
        "  outputs = Dense(n_classes, activation=tf.keras.activations.softmax)(x)\n",
        "  # optimizer\n",
        "  opt = tf.keras.optimizers.Nadam(learning_rate=learning_rate, \n",
        "                                  beta_1=0.9, beta_2=0.999,\n",
        "                                  epsilon=None, decay=0.99)\n",
        "  # when we compile the model we must specify inputs and outputs\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs],\n",
        "                        name=f'cnn_model_{\"%02d\" % time()}')\n",
        "  # compile the model\n",
        "  model.compile(loss='sparse_categorical_crossentropy',\n",
        "                optimizer=opt, \n",
        "                metrics=['categorical_accuracy'])\n",
        "  \n",
        "  # Generate an ASCII representation of the architecture\n",
        "  print(model.summary())\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "xMfh1YU0fDF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fun_model = build_sequential_model(**model_arguments)\n",
        "# generate a graphical representation of the neural network\n",
        "plot_model(seq_model, show_shapes=True, show_layer_names=True, to_file='fun_model.png')\n",
        "# display the graphical representation\n",
        "Image('fun_model.png')"
      ],
      "metadata": {
        "id": "S4AYXmNBamPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_parallel_functional_model(conv_filters,\n",
        "                           conv_size,\n",
        "                           dense_layers,\n",
        "                           image_size=(28, 28, 1),\n",
        "                           learning_rate=1e-3,\n",
        "                           n_classes=10,\n",
        "                           activation='selu'):\n",
        "  # define the input tensor\n",
        "  inputs = Input(image_size)\n",
        "\n",
        "  x = inputs\n",
        "  # construct the convolutional block\n",
        "  for (filters, kernel) in zip(conv_filters, conv_size):\n",
        "    # here we keep track of the input of each block\n",
        "    ins = x\n",
        "    # there are two paths through which the data and gradient can flow\n",
        "    # 1st path is x:\n",
        "    x = None # TODO\n",
        "    x = None # TODO\n",
        "    # 2nd path is y:\n",
        "    y = None # TODO\n",
        "    y = None # TODO\n",
        "    # both paths' outputs are concatenated across the filter dimension\n",
        "    x = Concatenate()([x, y])\n",
        "    # and then an additional convolution that reduces the total filter dimension\n",
        "    # is performed\n",
        "    x = Conv2D(filters=filters, kernel_size=(1, 1), activation=activation)(x)\n",
        "\n",
        "  # flatten\n",
        "  x = Flatten()(x)\n",
        "  # construct the dense part\n",
        "  for neurons in dense_layers:\n",
        "   x = Dense(neurons, activation=activation)(x)\n",
        "  # classification output\n",
        "  outputs = Dense(n_classes, activation=tf.keras.activations.softmax)(x)\n",
        "\n",
        "  opt = tf.keras.optimizers.Nadam(learning_rate=learning_rate, \n",
        "                                  beta_1=0.9, beta_2=0.999,\n",
        "                                  epsilon=None, decay=0.99)\n",
        "  # build the model\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=outputs,\n",
        "                        name=f'cnn_model_{\"%02d\" % time()}')\n",
        "  # compile the model\n",
        "  model.compile(loss='sparse_categorical_crossentropy',\n",
        "                optimizer=opt, \n",
        "                metrics=['categorical_accuracy'])\n",
        "  \n",
        "  # Generate an ASCII representation of the architecture\n",
        "  print(model.summary())\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "Ju_3zgAClSaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "par_model = build_parallel_functional_model(**model_arguments)\n",
        "# generate a graphical representation of the neural network\n",
        "plot_model(par_model, show_shapes=True, show_layer_names=True, to_file='par_model.png')\n",
        "# display the graphical representation\n",
        "Image('par_model.png')"
      ],
      "metadata": {
        "id": "Ptf13Mmjlzr4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}